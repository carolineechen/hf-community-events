{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJH7LMoESD-G",
        "outputId": "f03c1df5-d4e6-43e3-e17b-4413624ca845"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/634.9 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.7/634.9 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m634.9/634.9 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U keras_cv\n",
        "!pip install -q -U tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-P8fen9SXf8",
        "outputId": "6835da1c-feab-4fc0-92b0-5ba6b396054f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You do not have Waymo Open Dataset installed, so KerasCV Waymo metrics are not available.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "from keras_cv.models.stable_diffusion.clip_tokenizer import SimpleTokenizer\n",
        "from keras_cv.models.stable_diffusion.diffusion_model import DiffusionModel\n",
        "from keras_cv.models.stable_diffusion.image_encoder import ImageEncoder\n",
        "from keras_cv.models.stable_diffusion.noise_scheduler import NoiseScheduler\n",
        "from keras_cv.models.stable_diffusion.text_encoder import TextEncoder"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Go8ggvOFSwhW"
      },
      "source": [
        "## Basic Cluster Setup\n",
        "* Install Runhouse and latest SkyPilot version\n",
        "* Set up LambdaLabs or BYO credentials\n",
        "* Instantiate and launch cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAH5sSJtS2pY"
      },
      "outputs": [],
      "source": [
        "!pip install runhouse\n",
        "!pip install git+https://github.com/skypilot-org/skypilot.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QBbxVwVzDrS",
        "outputId": "7368932a-7d5f-4a69-96cf-d599d221baa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO | 2023-03-10 04:26:46,264 | No auth token provided, so not using RNS API to save and load configs\n"
          ]
        }
      ],
      "source": [
        "import runhouse as rh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktYqyqbnzfaF"
      },
      "source": [
        "### Option 1: On Demand Cluster that spins up/down for you"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtGDE1vPTOsI"
      },
      "outputs": [],
      "source": [
        "# To see instructions on how to set up cloud credentials. Skip if using your own cluster\n",
        "!sky check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF89-2tOyw-9",
        "outputId": "7be2ce1d-7880-4b51-e3dd-6020b9ba30a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mSkyPilot collects usage data to improve its services. `setup` and `run` commands are not collected to ensure privacy.\n",
            "Usage logging can be disabled by setting the environment variable SKYPILOT_DISABLE_USAGE_COLLECTION=1.\u001b[0m\n",
            "Checking credentials to enable clouds for SkyPilot.\n",
            "  \u001b[31m\u001b[1mAWS: disabled\u001b[0m          \n",
            "    Reason: AWS credentials are not set. Run the following commands:\n",
            "      $ pip install boto3\n",
            "      $ aws configure\n",
            "    For more info: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html\n",
            "  \u001b[31m\u001b[1mAzure: disabled\u001b[0m          \n",
            "    Reason: ~/.azure/msal_token_cache.json does not exist. Run the following commands:\n",
            "      $ az login\n",
            "      $ az account set -s <subscription_id>\n",
            "    For more info: https://docs.microsoft.com/en-us/cli/azure/get-started-with-azure-cli\n",
            "  \u001b[31m\u001b[1mGCP: disabled\u001b[0m          \n",
            "    Reason: GCP tools are not installed or credentials are not set. Run the following commands:\n",
            "      $ pip install google-api-python-client\n",
            "      $ conda install -c conda-forge google-cloud-sdk -y\n",
            "      $ gcloud init\n",
            "      $ gcloud auth application-default login\n",
            "    For more info: https://skypilot.readthedocs.io/en/latest/getting-started/installation.html\n",
            "  \u001b[32m\u001b[1mLambda: enabled\u001b[0m          \n",
            "\n",
            "SkyPilot will use only the enabled clouds to run tasks. To change this, configure cloud credentials, and run \u001b[1msky check\u001b[0m.\n",
            "\u001b[2mIf any problems remain, please file an issue at https://github.com/skypilot-org/skypilot/issues/new\u001b[0m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# For Lambda Labs\n",
        "# First get your API key from https://cloud.lambdalabs.com/api-keys\n",
        "# and create the file lambda_keys with the following line\n",
        "# api_key = [YOUR API KEY]\n",
        "\n",
        "!mkdir ~/.lambda_cloud/\n",
        "!mv lambda_keys ~/.lambda_cloud/lambda_keys\n",
        "!sky check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re48yUFGzXuU"
      },
      "outputs": [],
      "source": [
        "# Launch on-demand Lambda cluster\n",
        "gpu = rh.cluster(name='rh-a100', instance_type='A100:1', provider='lambda')\n",
        "gpu.up_if_not()\n",
        "\n",
        "# set amount of time (min) of inactivity to shut down cluster, or -1 to keep up indefinitely (Default: 30 min)\n",
        "gpu.autostop_mins = -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKwH46f6zoij"
      },
      "source": [
        "### Option 2: Bring-your-own cluster, by passing in IPs and SSH creds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jyzbrHgM1pW",
        "outputId": "4ba94482-b35f-4975-9131-bf9e77938d93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO | 2023-03-09 14:48:24,824 | Running command on byo-lambda: ray start --head\n",
            "INFO | 2023-03-09 14:48:29,587 | Running command on byo-lambda: mkdir -p ~/.rh; touch ~/.rh/cluster_config.yaml; echo '{\"name\": \"~/byo-lambda\", \"resource_type\": \"cluster\", \"resource_subtype\": \"Cluster\", \"ips\": [\"132.145.193.245\"], \"ssh_creds\": {\"ssh_user\": \"ubuntu\", \"ssh_private_key\": \"~/.ssh/id_rsa\"}}' > ~/.rh/cluster_config.yaml\n"
          ]
        }
      ],
      "source": [
        "# Uncomment for bring-your-own cluster. This can be a cluster spun up by Lambda Labs\n",
        "# gpu = rh.cluster(name='byo-lambda', ips=['<ip_address>'],\n",
        "#                  ssh_creds={'ssh_user':'ubuntu', 'ssh_private_key': '~/.ssh/id_rsa'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAv4A-DlzGJm"
      },
      "source": [
        "### Set up Tensorflow with GPU Support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM9XjvwNz-OG"
      },
      "source": [
        "Install Tensorflow, and check that it has GPU set up properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-OGGWFCz9dA"
      },
      "outputs": [],
      "source": [
        "command = \"conda install -y -c conda-forge cudatoolkit=11.2.2 cudnn=8.1.0; \\\n",
        "            mkdir -p $CONDA_PREFIX/etc/conda/activate.d; \\\n",
        "            echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/' > $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh; \\\n",
        "            python3 pip install tensorflow\"\n",
        "gpu.run([command])\n",
        "gpu.restart_grpc_server()  # restart server to load env variables set above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmvQ6IbdzWyF",
        "outputId": "2ff4a13e-a6dc-4199-ef0f-543f78661937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO | 2023-03-10 04:36:46,362 | Running command on rh-a100: python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n",
            "2023-03-10 04:36:47.825877: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-10 04:36:48.543604: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/ubuntu/miniconda3/lib/:/home/ubuntu/miniconda3/lib/\n",
            "2023-03-10 04:36:48.543662: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/ubuntu/miniconda3/lib/:/home/ubuntu/miniconda3/lib/\n",
            "2023-03-10 04:36:48.543668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-10 04:36:49.482281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-03-10 04:36:49.488270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-03-10 04:36:49.489895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(0,\n",
              "  \"2023-03-10 04:36:47.825877: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\\n2023-03-10 04:36:48.543604: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/ubuntu/miniconda3/lib/:/home/ubuntu/miniconda3/lib/\\n2023-03-10 04:36:48.543662: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/ubuntu/miniconda3/lib/:/home/ubuntu/miniconda3/lib/\\n2023-03-10 04:36:48.543668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\\n2023-03-10 04:36:49.482281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\\n2023-03-10 04:36:49.488270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\\n2023-03-10 04:36:49.489895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\\n\",\n",
              "  '')]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpu.run(['python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices(\\'GPU\\'))\"'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc7WywngzLlV"
      },
      "source": [
        "## Dreambooth Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrJ_1AGUSKIk"
      },
      "source": [
        "### Download the instance and class images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "4WoWyRhMSHzE",
        "outputId": "e3a57304-2290-4ae5-a4d4-d86b4e58074d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/instance-images.tar.gz\n",
            "5556967/5556967 [==============================] - 0s 0us/step\n",
            "Downloading data from https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/class-images.tar.gz\n",
            "9093120/9093120 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/root/.keras/datasets/class-images'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.keras.utils.get_file(\n",
        "    origin=\"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/instance-images.tar.gz\",\n",
        "    untar=True\n",
        ")\n",
        "tf.keras.utils.get_file(\n",
        "    origin=\"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/class-images.tar.gz\",\n",
        "    untar=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IasbTvj5SRUJ",
        "outputId": "461e1404-e188-4a97-fa3e-524479c59444"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO | 2023-03-10 04:36:54,461 | Creating new file folder: /root/.keras/datasets/instance-images\n",
            "INFO | 2023-03-10 04:36:54,474 | Copying folder from file:///root/.keras/datasets/instance-images to: rh-a100, with path: ~/.keras/datasets/instance-images\n",
            "INFO | 2023-03-10 04:36:54,476 | Creating new ssh folder: .keras/datasets/instance-images\n",
            "INFO | 2023-03-10 04:36:54,536 | Opening SSH connection to 150.136.66.243, port 22\n",
            "INFO | 2023-03-10 04:36:54,563 | [conn=0] Connected to SSH server at 150.136.66.243, port 22\n",
            "INFO | 2023-03-10 04:36:54,565 | [conn=0]   Local address: 172.28.0.12, port 59042\n",
            "INFO | 2023-03-10 04:36:54,566 | [conn=0]   Peer address: 150.136.66.243, port 22\n",
            "INFO | 2023-03-10 04:36:54,675 | [conn=0] Beginning auth for user ubuntu\n",
            "INFO | 2023-03-10 04:36:54,776 | [conn=0] Auth for user ubuntu succeeded\n",
            "INFO | 2023-03-10 04:36:54,780 | [conn=0, chan=0] Requesting new SSH session\n",
            "INFO | 2023-03-10 04:36:55,181 | [conn=0, chan=0]   Subsystem: sftp\n",
            "INFO | 2023-03-10 04:36:55,211 | [conn=0, chan=0] Starting SFTP client\n",
            "INFO | 2023-03-10 04:36:55,967 | Creating new file folder: /root/.keras/datasets/class-images\n",
            "INFO | 2023-03-10 04:36:55,969 | Copying folder from file:///root/.keras/datasets/class-images to: rh-a100, with path: ~/.keras/datasets/class-images\n",
            "INFO | 2023-03-10 04:36:55,971 | Creating new ssh folder: .keras/datasets/class-images\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<runhouse.rns.folders.folder.Folder at 0x7f91943f9190>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "instance_images_root = '~/.keras/datasets/instance-images'\n",
        "class_images_root = '~/.keras/datasets/class-images'\n",
        "\n",
        "# sync images to the cluster using Runhouse\n",
        "rh.folder(path=instance_images_root).to(system=gpu, path=instance_images_root)\n",
        "rh.folder(path=class_images_root).to(system=gpu, path=class_images_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lnz43pfaUv6",
        "outputId": "dd0a0b5b-a8e5-49be-dcdb-d4feb0c10bbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO | 2023-03-10 04:36:59,351 | Writing out function function to /content/get_image_paths_fn.py as functions serialized in notebooks are brittle. Please make sure the function does not rely on any local variables, including imports (which should be moved inside the function body).\n",
            "INFO | 2023-03-10 04:36:59,356 | Setting up Function on cluster.\n",
            "INFO | 2023-03-10 04:36:59,359 | Creating new file folder: /content\n",
            "INFO | 2023-03-10 04:36:59,362 | Copying local package content to cluster <rh-a100>\n",
            "INFO | 2023-03-10 04:36:59,365 | Creating new ssh folder: content\n",
            "INFO | 2023-03-10 04:37:02,209 | Installing packages on cluster rh-a100: ['./']\n",
            "INFO | 2023-03-10 04:37:02,267 | Function setup complete.\n"
          ]
        }
      ],
      "source": [
        "def get_image_paths(folder):\n",
        "    from pathlib import Path\n",
        "    import os\n",
        "\n",
        "    abs_folder = Path(folder).expanduser()\n",
        "    files = os.listdir(abs_folder)\n",
        "    files = [os.path.join(abs_folder, file) for file in files]\n",
        "    return files\n",
        "\n",
        "# Get image paths on the cluster\n",
        "get_image_paths_gpu = rh.function(fn=get_image_paths).to(system=gpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYpFkZjQdpFT",
        "outputId": "f2f9f57a-287d-493f-c725-89225588f4e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO | 2023-03-10 04:37:05,146 | Running get_image_paths via gRPC\n",
            "INFO | 2023-03-10 04:37:05,199 | Time to send message: 0.05 seconds\n",
            "INFO | 2023-03-10 04:37:05,200 | Running get_image_paths via gRPC\n",
            "INFO | 2023-03-10 04:37:05,306 | Time to send message: 0.1 seconds\n",
            "['/home/ubuntu/.keras/datasets/instance-images/alvan-nee-bQaAJCbNq3g-unsplash.jpeg', '/home/ubuntu/.keras/datasets/instance-images/alvan-nee-eoqnr8ikwFE-unsplash.jpeg', '/home/ubuntu/.keras/datasets/instance-images/alvan-nee-9M0tSjb-cpA-unsplash.jpeg', '/home/ubuntu/.keras/datasets/instance-images/alvan-nee-brFsZ7qszSY-unsplash.jpeg', '/home/ubuntu/.keras/datasets/instance-images/alvan-nee-Id1DBHv4fbg-unsplash.jpeg']\n",
            "['/home/ubuntu/.keras/datasets/class-images/cae1100cdc58a2436697ba178cd3deaed0b43064.jpg', '/home/ubuntu/.keras/datasets/class-images/9c54d0af0a22d05914b5894b55817fa33eac80d4.jpg', '/home/ubuntu/.keras/datasets/class-images/27b9d79bdc218d483c365e40961b77106d986815.jpg', '/home/ubuntu/.keras/datasets/class-images/84b4ef76f898fdf844f260ed916471c9c93bb2c7.jpg', '/home/ubuntu/.keras/datasets/class-images/16a5b5d8199584157ab26e1593605dbdf6860882.jpg']\n"
          ]
        }
      ],
      "source": [
        "instance_image_paths = get_image_paths_gpu(instance_images_root)\n",
        "class_image_paths = get_image_paths_gpu(class_images_root)\n",
        "\n",
        "print(instance_image_paths[:5])\n",
        "print(class_image_paths[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-ukIxKzSePX"
      },
      "source": [
        "### Prepare captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV_u86JkSb_R"
      },
      "outputs": [],
      "source": [
        "# match the number of instance images we're using\n",
        "new_instance_image_paths = []\n",
        "for index in range(len(class_image_paths)):\n",
        "    instance_image = instance_image_paths[index % len(instance_image_paths)]\n",
        "    new_instance_image_paths.append(instance_image)\n",
        "\n",
        "# repeat the prompts / captions per images. \n",
        "unique_id = \"sks\"\n",
        "class_label = \"dog\"\n",
        "\n",
        "instance_prompt = f\"a photo of {unique_id} {class_label}\" \n",
        "instance_prompts = [instance_prompt] * len(new_instance_image_paths)\n",
        "\n",
        "class_prompt = f\"a photo of {class_label}\"\n",
        "class_prompts = [class_prompt] * len(class_image_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqGhbeKVVxLv",
        "outputId": "249c9a55-5ac3-462c-d48d-13165ba76008"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/openai/CLIP/blob/main/clip/bpe_simple_vocab_16e6.txt.gz?raw=true\n",
            "1356917/1356917 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# tokenize the text\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "padding_token = 49407\n",
        "max_prompt_length = 77\n",
        "tokenizer = SimpleTokenizer() \n",
        "\n",
        "def process_text(caption):\n",
        "    tokens = tokenizer.encode(caption)\n",
        "    tokens = tokens + [padding_token] * (max_prompt_length - len(tokens))\n",
        "    return np.array(tokens)\n",
        "\n",
        "tokenized_texts = np.empty((len(instance_prompts) + len(class_prompts), max_prompt_length))\n",
        "for i, caption in enumerate(itertools.chain(instance_prompts, class_prompts)):\n",
        "    tokenized_texts[i] = process_text(caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nScuxE5aDPNZ"
      },
      "outputs": [],
      "source": [
        "# pre-compute the text embeddings to save some memory during training.\n",
        "# this should be run on a GPU\n",
        "def encode_text(tokenized_texts, max_prompt_length):\n",
        "    import tensorflow as tf\n",
        "    from keras_cv.models.stable_diffusion.text_encoder import TextEncoder\n",
        "\n",
        "    POS_IDS = tf.convert_to_tensor([list(range(max_prompt_length))], dtype=tf.int32)\n",
        "    text_encoder = TextEncoder(max_prompt_length)\n",
        "\n",
        "    gpus = tf.config.list_logical_devices(\"GPU\")\n",
        "\n",
        "    # Ensure the computation takes place on a GPU.\n",
        "    with tf.device(gpus[0].name):\n",
        "        embedded_text = text_encoder(\n",
        "            [tf.convert_to_tensor(tokenized_texts), POS_IDS], training=False\n",
        "        ).numpy()\n",
        "\n",
        "    del text_encoder\n",
        "    return embedded_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgY-ZpGeXT42",
        "outputId": "18d95b86-5eb7-4b2c-8316-8a8ce2118150"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO | 2023-03-10 05:03:54,468 | Writing out function function to /content/encode_text_fn.py as functions serialized in notebooks are brittle. Please make sure the function does not rely on any local variables, including imports (which should be moved inside the function body).\n",
            "INFO | 2023-03-10 05:03:54,475 | Setting up Function on cluster.\n",
            "INFO | 2023-03-10 05:03:54,479 | Creating new file folder: /content\n",
            "INFO | 2023-03-10 05:03:54,485 | Copying local package content to cluster <rh-a100>\n",
            "INFO | 2023-03-10 05:03:54,487 | Creating new ssh folder: content\n",
            "INFO | 2023-03-10 05:03:55,490 | Installing packages on cluster rh-a100: ['tensorflow', 'keras_cv', 'imutils', 'opencv-python', './']\n",
            "INFO | 2023-03-10 05:04:01,325 | Function setup complete.\n",
            "INFO | 2023-03-10 05:04:01,327 | Running encode_text via gRPC\n",
            "INFO | 2023-03-10 05:04:17,059 | Time to send message: 15.73 seconds\n"
          ]
        }
      ],
      "source": [
        "# send function to be run on GPU defined above\n",
        "encode_text_gpu = rh.function(fn=encode_text, system=gpu, reqs=['tensorflow', 'keras_cv', 'imutils', 'opencv-python'])\n",
        "embedded_text = encode_text_gpu(tokenized_texts, max_prompt_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DecO3tjME5R7",
        "outputId": "77cd0b21-53df-401e-c0ce-2a2763a8fa65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(400, 77, 768)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedded_text.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckAfOqmKSgB5"
      },
      "source": [
        "### Prepare the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmzDaBTrijEV"
      },
      "outputs": [],
      "source": [
        "def assemble_dataset(instance_paths, class_paths, embedded_texts, save_path, batch_size=1):\n",
        "    import keras_cv\n",
        "    import tensorflow as tf\n",
        "    import os\n",
        "    from pathlib import Path\n",
        "\n",
        "    resolution = 512\n",
        "    auto = tf.data.AUTOTUNE\n",
        "\n",
        "    augmenter = keras_cv.layers.Augmenter(\n",
        "        layers=[\n",
        "            keras_cv.layers.CenterCrop(resolution, resolution),\n",
        "            keras_cv.layers.RandomFlip(),\n",
        "            tf.keras.layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "    def process_image(image_path, tokenized_text):\n",
        "        image = tf.io.read_file(image_path)\n",
        "        image = tf.io.decode_png(image, 3)\n",
        "        image = tf.image.resize(image, (resolution, resolution))\n",
        "        return image, tokenized_text\n",
        "\n",
        "\n",
        "    def apply_augmentation(image_batch, embedded_tokens):\n",
        "        return augmenter(image_batch), embedded_tokens\n",
        "\n",
        "\n",
        "    def prepare_dict(instance_only=True):\n",
        "        def fn(image_batch, embedded_tokens):\n",
        "            if instance_only:\n",
        "                batch_dict = {\n",
        "                    \"instance_images\": image_batch,\n",
        "                    \"instance_embedded_texts\": embedded_tokens,\n",
        "                }\n",
        "                return batch_dict\n",
        "            else:\n",
        "                batch_dict = {\n",
        "                    \"class_images\": image_batch,\n",
        "                    \"class_embedded_texts\": embedded_tokens,\n",
        "                }\n",
        "                return batch_dict\n",
        "        return fn\n",
        "\n",
        "\n",
        "    def assemble(image_paths, embedded_texts, instance_only, batch_size):  \n",
        "        dataset = tf.data.Dataset.from_tensor_slices(\n",
        "            (image_paths, embedded_texts)\n",
        "        )\n",
        "        dataset = dataset.map(process_image, num_parallel_calls=auto)\n",
        "        dataset = dataset.shuffle(5, reshuffle_each_iteration=True)\n",
        "        dataset = dataset.batch(batch_size)\n",
        "        dataset = dataset.map(apply_augmentation, num_parallel_calls=auto)\n",
        "\n",
        "        prepare_dict_fn = prepare_dict(instance_only=instance_only)\n",
        "        dataset = dataset.map(prepare_dict_fn, num_parallel_calls=auto)\n",
        "        return dataset\n",
        "    \n",
        "    instance_dataset = assemble(instance_paths, embedded_texts[:len(instance_paths)], True, batch_size)\n",
        "    class_dataset = assemble(class_paths, embedded_texts[len(instance_paths):], False, batch_size)\n",
        "    train_dataset = tf.data.Dataset.zip((instance_dataset, class_dataset))\n",
        "\n",
        "    abs_path = str(Path(save_path).expanduser())\n",
        "    tf.data.Dataset.save(train_dataset, abs_path)\n",
        "    return abs_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bugtQkerFSF",
        "outputId": "e7d97499-9ce2-40db-98c8-d4f8c9be01f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO | 2023-03-10 04:38:14,844 | Writing out function function to /content/assemble_dataset_fn.py as functions serialized in notebooks are brittle. Please make sure the function does not rely on any local variables, including imports (which should be moved inside the function body).\n",
            "INFO | 2023-03-10 04:38:14,853 | Setting up Function on cluster.\n",
            "INFO | 2023-03-10 04:38:14,857 | Creating new file folder: /content\n",
            "INFO | 2023-03-10 04:38:14,859 | Copying local package content to cluster <rh-a100>\n",
            "INFO | 2023-03-10 04:38:14,863 | Creating new ssh folder: content\n",
            "INFO | 2023-03-10 04:38:15,211 | Installing packages on cluster rh-a100: ['./']\n",
            "INFO | 2023-03-10 04:38:15,271 | Function setup complete.\n",
            "INFO | 2023-03-10 04:38:15,275 | Running assemble_dataset via gRPC\n",
            "INFO | 2023-03-10 04:38:29,864 | Time to send message: 14.48 seconds\n"
          ]
        }
      ],
      "source": [
        "assemble_dataset_gpu = rh.function(fn=assemble_dataset).to(system=gpu)\n",
        "save_data_path = '~/.keras/datasets/train_dataset'\n",
        "train_dataset_path = assemble_dataset_gpu(new_instance_image_paths, class_image_paths, embedded_text, save_data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAwiAKQXYX55"
      },
      "source": [
        "## Dreambooth Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeJ0vRheYAiK"
      },
      "outputs": [],
      "source": [
        "# To be run on GPU\n",
        "def train_dreambooth(resolution, max_prompt_length, use_mp, opt_args, dataset_path, ckpt_path):\n",
        "    import math\n",
        "    import os\n",
        "    import tensorflow as tf\n",
        "    import tensorflow.experimental.numpy as tnp\n",
        "\n",
        "    from keras_cv.models.stable_diffusion.diffusion_model import DiffusionModel\n",
        "    from keras_cv.models.stable_diffusion.image_encoder import ImageEncoder\n",
        "    from keras_cv.models.stable_diffusion.noise_scheduler import NoiseScheduler\n",
        "\n",
        "    tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "    class DreamBoothTrainer(tf.keras.Model):\n",
        "        # Reference: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth.py\n",
        "        def __init__(\n",
        "            self,\n",
        "            diffusion_model,\n",
        "            vae,\n",
        "            noise_scheduler,\n",
        "            use_mixed_precision=False,\n",
        "            prior_loss_weight=1.0,\n",
        "            max_grad_norm=1.0,\n",
        "            **kwargs\n",
        "        ):\n",
        "            super().__init__(**kwargs)\n",
        "\n",
        "            self.diffusion_model = diffusion_model\n",
        "            self.vae = vae\n",
        "            self.noise_scheduler = noise_scheduler\n",
        "            self.prior_loss_weight = prior_loss_weight\n",
        "            self.max_grad_norm = max_grad_norm\n",
        "\n",
        "            self.use_mixed_precision = use_mixed_precision\n",
        "            self.vae.trainable = False\n",
        "\n",
        "        def train_step(self, inputs):\n",
        "            instance_batch = inputs[0]\n",
        "            class_batch = inputs[1]\n",
        "\n",
        "            instance_images = instance_batch[\"instance_images\"]\n",
        "            instance_embedded_text = instance_batch[\"instance_embedded_texts\"]\n",
        "            class_images = class_batch[\"class_images\"]\n",
        "            class_embedded_text = class_batch[\"class_embedded_texts\"]\n",
        "\n",
        "            images = tf.concat([instance_images, class_images], 0)\n",
        "            embedded_texts = tf.concat([instance_embedded_text, class_embedded_text], 0)\n",
        "            batch_size = tf.shape(images)[0]\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Project image into the latent space and sample from it.\n",
        "                latents = self.sample_from_encoder_outputs(self.vae(images, training=False))\n",
        "                # Know more about the magic number here:\n",
        "                # https://keras.io/examples/generative/fine_tune_via_textual_inversion/\n",
        "                latents = latents * 0.18215\n",
        "\n",
        "                # Sample noise that we'll add to the latents.\n",
        "                noise = tf.random.normal(tf.shape(latents))\n",
        "\n",
        "                # Sample a random timestep for each image.\n",
        "                timesteps = tnp.random.randint(\n",
        "                    0, self.noise_scheduler.train_timesteps, (batch_size,)\n",
        "                )\n",
        "\n",
        "                # Add noise to the latents according to the noise magnitude at each timestep\n",
        "                # (this is the forward diffusion process).\n",
        "                noisy_latents = self.noise_scheduler.add_noise(\n",
        "                    tf.cast(latents, noise.dtype), noise, timesteps\n",
        "                )\n",
        "\n",
        "                # Get the target for loss depending on the prediction type\n",
        "                # just the sampled noise for now.\n",
        "                target = noise  # noise_schedule.predict_epsilon == True\n",
        "\n",
        "                # Predict the noise residual and compute loss.\n",
        "                timestep_embedding = tf.map_fn(\n",
        "                    lambda t: self.get_timestep_embedding(t), timesteps, dtype=tf.float32\n",
        "                )\n",
        "                model_pred = self.diffusion_model(\n",
        "                    [noisy_latents, timestep_embedding, embedded_texts], training=True\n",
        "                )\n",
        "                loss = self.compute_loss(target, model_pred)\n",
        "                if self.use_mixed_precision:\n",
        "                    loss = self.optimizer.get_scaled_loss(loss)\n",
        "\n",
        "            # Update parameters of the diffusion model.\n",
        "            trainable_vars = self.diffusion_model.trainable_variables\n",
        "            gradients = tape.gradient(loss, trainable_vars)\n",
        "            if self.use_mixed_precision:\n",
        "                gradients = self.optimizer.get_unscaled_gradients(gradients)\n",
        "            gradients = [tf.clip_by_norm(g, self.max_grad_norm) for g in gradients]\n",
        "            self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "            return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "        def get_timestep_embedding(self, timestep, dim=320, max_period=10000):\n",
        "            half = dim // 2\n",
        "            log_max_preiod = tf.math.log(tf.cast(max_period, tf.float32))\n",
        "            freqs = tf.math.exp(\n",
        "                -log_max_preiod * tf.range(0, half, dtype=tf.float32) / half\n",
        "            )\n",
        "            args = tf.convert_to_tensor([timestep], dtype=tf.float32) * freqs\n",
        "            embedding = tf.concat([tf.math.cos(args), tf.math.sin(args)], 0)\n",
        "            return embedding\n",
        "\n",
        "        def sample_from_encoder_outputs(self, outputs):\n",
        "            mean, logvar = tf.split(outputs, 2, axis=-1)\n",
        "            logvar = tf.clip_by_value(logvar, -30.0, 20.0)\n",
        "            std = tf.exp(0.5 * logvar)\n",
        "            sample = tf.random.normal(tf.shape(mean), dtype=mean.dtype)\n",
        "            return mean + std * sample\n",
        "\n",
        "        def compute_loss(self, target, model_pred):\n",
        "            # Chunk the noise and model_pred into two parts and compute the loss\n",
        "            # on each part separately.\n",
        "            # Since the first half of the inputs has instance samples and the second half\n",
        "            # has class samples, we do the chunking accordingly. \n",
        "            model_pred, model_pred_prior = tf.split(model_pred, num_or_size_splits=2, axis=0)\n",
        "            target, target_prior = tf.split(target, num_or_size_splits=2, axis=0)\n",
        "\n",
        "            # Compute instance loss.\n",
        "            loss = self.compiled_loss(target, model_pred)\n",
        "\n",
        "            # Compute prior loss.\n",
        "            prior_loss = self.compiled_loss(target_prior, model_pred_prior)\n",
        "\n",
        "            # Add the prior loss to the instance loss.\n",
        "            loss = loss + self.prior_loss_weight * prior_loss\n",
        "            return loss\n",
        "\n",
        "        def save_weights(self, filepath, overwrite=True, save_format=None, options=None):\n",
        "            # Overriding this method will allow us to use the `ModelCheckpoint`\n",
        "            # callback directly with this trainer class. In this case, it will\n",
        "            # only checkpoint the `diffusion_model` since that's what we're training\n",
        "            # during fine-tuning.\n",
        "            self.diffusion_model.save_weights(\n",
        "                filepath=filepath,\n",
        "                overwrite=overwrite,\n",
        "                save_format=save_format,\n",
        "                options=options,\n",
        "            )\n",
        "\n",
        "    image_encoder = ImageEncoder(resolution, resolution)\n",
        "    diffusion_model = DiffusionModel(resolution, resolution, max_prompt_length)\n",
        "    optimizer = tf.keras.optimizers.experimental.AdamW(\n",
        "        learning_rate=opt_args['lr'],\n",
        "        weight_decay=opt_args['weight_decay'],\n",
        "        beta_1=opt_args['beta_1'],\n",
        "        beta_2=opt_args['beta_2'],\n",
        "        epsilon=opt_args['epsilon'],\n",
        "    )\n",
        "\n",
        "    dreambooth_trainer = DreamBoothTrainer(\n",
        "        diffusion_model=diffusion_model,\n",
        "        vae=tf.keras.Model(\n",
        "                image_encoder.input,\n",
        "                image_encoder.layers[-2].output,\n",
        "            ),\n",
        "        noise_scheduler=NoiseScheduler(),\n",
        "        use_mixed_precision=use_mp,\n",
        "    )\n",
        "    dreambooth_trainer.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "    train_dataset = tf.data.Dataset.load(dataset_path)\n",
        "    num_update_steps_per_epoch = train_dataset.cardinality()\n",
        "    max_train_steps = 800\n",
        "    epochs =  math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
        "    print(f\"Training for {epochs} epochs.\")\n",
        "\n",
        "    ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        ckpt_path,\n",
        "        save_weights_only=True,\n",
        "        monitor=\"loss\",\n",
        "        mode=\"min\",\n",
        "    )\n",
        "    gpus = tf.config.list_logical_devices(\"GPU\")\n",
        "\n",
        "    # Ensure the computation takes place on a GPU.\n",
        "    with tf.device(gpus[0].name):\n",
        "        dreambooth_trainer.fit(train_dataset, epochs=epochs, callbacks=[ckpt_callback])\n",
        "    return os.path.abspath(ckpt_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKpFKJofYezN",
        "outputId": "cc86690f-f371-4684-ae44-7255a8aa9d77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO | 2023-03-10 05:41:23,101 | Writing out function function to /content/train_dreambooth_fn.py as functions serialized in notebooks are brittle. Please make sure the function does not rely on any local variables, including imports (which should be moved inside the function body).\n",
            "INFO | 2023-03-10 05:41:23,113 | Setting up Function on cluster.\n",
            "INFO | 2023-03-10 05:41:23,116 | Creating new file folder: /content\n",
            "INFO | 2023-03-10 05:41:23,119 | Copying local package content to cluster <rh-a100>\n",
            "INFO | 2023-03-10 05:41:23,123 | Creating new ssh folder: content\n",
            "INFO | 2023-03-10 05:41:23,397 | Installing packages on cluster rh-a100: ['./']\n",
            "INFO | 2023-03-10 05:41:23,550 | Function setup complete.\n",
            "INFO | 2023-03-10 05:41:23,556 | Running train_dreambooth via gRPC\n",
            "INFO | 2023-03-10 05:49:35,949 | Time to send message: 492.39 seconds\n"
          ]
        }
      ],
      "source": [
        "use_mp = True # Set it to False if you're not using a GPU with tensor cores.\n",
        "resolution = 512\n",
        "model_save_path = '~/.keras/models/dreambooth_trainer'\n",
        "\n",
        "# These hyperparameters come from this tutorial by Hugging Face:\n",
        "# https://github.com/huggingface/diffusers/tree/main/examples/dreambooth\n",
        "optimizer_params = {\n",
        "    'lr': 5e-6,\n",
        "    'beta_1': 0.9,\n",
        "    'beta_2': 0.999,\n",
        "    'weight_decay': (1e-2,),\n",
        "    'epsilon': 1e-08,\n",
        "}\n",
        "ckpt_path = \"dreambooth-unet.h5\"\n",
        "\n",
        "# set up libdevice.10.bc to be discoverable by tensorflow\n",
        "gpu.run(['cp /usr/lib/cuda/nvvm/libdevice/libdevice.10.bc .'])\n",
        "train_dreambooth_gpu = rh.function(fn=train_dreambooth, system=gpu)\n",
        "ckpt_path_gpu = train_dreambooth_gpu(resolution, max_prompt_length, use_mp, optimizer_params, train_dataset_path, ckpt_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BmyiSyo0CZLo",
        "outputId": "1bf716fd-7597-4037-fa19-eca84d762471"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/home/ubuntu/dreambooth-unet.h5'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ckpt_path_gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-OZlP_wF2ZB"
      },
      "outputs": [],
      "source": [
        "# to terminate the instance from Colab, or you can go into lambdalabs website to manually terminate.\n",
        "# !sky down rh-a100"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.2 (main, Feb 12 2023, 06:26:18) [Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
